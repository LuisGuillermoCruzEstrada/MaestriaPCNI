{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMCWZ+haIXWNQG90R2YhXf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZlwsRej7WEe","executionInfo":{"status":"ok","timestamp":1730756322198,"user_tz":360,"elapsed":14080,"user":{"displayName":"LUIS GUILLERMO CRUZ ESTRADA","userId":"05182747514482678488"}},"outputId":"326b8335-1635-48b4-9a61-5a9ed1db40ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"]}],"source":["!pip install transformers pandas torch"]},{"cell_type":"code","source":[],"metadata":{"id":"QuLgq0W_76iN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importar las librerías necesarias\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n","import pandas as pd\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","# Cargar la base de datos\n","data = pd.read_csv('resultado.csv')  # Cambia esta ruta\n","\n","# Preprocesamiento: asignar clases múltiples\n","data['Activity'] = data['Activity'].apply(lambda x: x.split(', '))\n","\n","# Binarizar las etiquetas\n","label_binarizer = MultiLabelBinarizer()\n","data['Activity'] = label_binarizer.fit_transform(data['Activity']).tolist()\n","class_names = label_binarizer.classes_\n","\n","# Definir el Dataset\n","class PeptideDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_length=50):\n","        self.dataframe = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        sequence = row['Seqence']\n","        labels = torch.tensor(row['Activity'], dtype=torch.float)\n","        encoding = self.tokenizer(sequence, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n","\n","        item = {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': labels\n","        }\n","        return item\n","\n","# Inicializar el tokenizador y el modelo de Hugging Face\n","tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"Rostlab/prot_bert_bfd\", num_labels=len(class_names))\n","\n","# Reemplazar la capa de clasificación con una nueva capa usando Sigmoid\n","model.classifier = torch.nn.Sequential(\n","    torch.nn.Linear(model.config.hidden_size, len(class_names)),\n","    torch.nn.Sigmoid()\n",")\n","\n","# Dividir los datos en entrenamiento y validación\n","train_data = data.sample(frac=0.8, random_state=42)\n","val_data = data.drop(train_data.index)\n","\n","# Crear datasets personalizados\n","train_dataset = PeptideDataset(train_data, tokenizer)\n","val_dataset = PeptideDataset(val_data, tokenizer)\n","\n","# Crear DataLoaders\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=64)\n","\n","# Configurar el optimizador\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Enviar el modelo a la GPU si está disponible\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Bucle de entrenamiento\n","epochs = 3\n","for epoch in range(epochs):\n","    model.train()\n","    total_train_loss = 0\n","\n","    for batch in train_dataloader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        model.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        loss = torch.nn.BCELoss()(outputs.logits, labels)\n","        total_train_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_train_loss / len(train_dataloader)}\")\n","\n","    # Evaluación\n","    model.eval()\n","    total_eval_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            loss = torch.nn.BCELoss()(outputs.logits, labels)\n","            total_eval_loss += loss.item()\n","\n","            # Predicciones\n","            predictions = (outputs.logits > 0.5).float()\n","            correct += (predictions == labels).all(dim=1).sum().item()\n","            total += labels.size(0)\n","\n","    avg_eval_loss = total_eval_loss / len(val_dataloader)\n","    accuracy = correct / total\n","    print(f\"Validation Loss: {avg_eval_loss}, Accuracy: {accuracy}\")"],"metadata":{"id":"Bsp22vC07aex","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e2ff314b-d3fd-4a7a-cc17-c7386c9d7c2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_7XbaDg5Gddt"},"execution_count":null,"outputs":[]}]}